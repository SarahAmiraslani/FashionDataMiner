# -*- coding: utf-8 -*-
"""OpsCons_Shein.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SCZ6ZaY42dDX6MBN7XEQgParDcGMAqbg
"""

import requests
import time
from bs4 import BeautifulSoup as Soup
import pandas as pd
import hashlib
import lxml
from datetime import datetime, date
import re
import ast
!pip install demjson
import demjson

def parse_sitemap(url, headers):
    # First of all we make a request to the specified url in the function parameters.
    resp = requests.get(url)
    # if we didn't get a valid response, return false 
    status = resp.status_code 
    if (status != 200):
        return False
    # Then we parse the content of the response using BeautifulSoup4.
    soup = Soup(resp.content, 'xml')

    #Then we look for either a urlset or a sitemapindex
    urls = soup.findAll('url')
    sitemaps = soup.findAll('sitemap')
    #create pandas dataframe
    new_list = ['Source'] + headers
    panda_out_total = pd.DataFrame([], columns=new_list)


    if not urls and not sitemaps:
        return False

    # Recursive call to the the function if sitemap contains sitemaps
    if sitemaps:
        for url in sitemaps:
            sitemap_url = url.find('loc').string
            panda_recursive = parse_sitemap(sitemap_url, headers)
            panda_out_total = pd.concat([panda_out_total, panda_recursive], ignore_index=True)

    # storage for later...
    out = []

    # Creates a hash of the parent sitemap for faster indexing
    hash_sitemap = hashlib.md5(str(url).encode('utf-8')).hexdigest()

    # Extract the keys we want
    for u in urls:
        values = [hash_sitemap]
        for head in headers:
            loc = None
            loc = u.find(head)
            if not loc:
                loc = 'None'
            else:
                loc = loc.string
            values.append(loc)
        out.append(values)
    
    # Creates a dataframe
    panda_out = pd.DataFrame(out, columns=new_list)

    # If recursive then merge recursive dataframe
    if not panda_out_total.empty:
        panda_out = pd.concat([panda_out, panda_out_total], ignore_index=True)

    #returns the dataframe
    return panda_out

#Do not run this code section if the CSV file is on the drive
#result = parse_sitemap("https://www.shein.com/sitemap-index.xml", ["loc"])
#result.to_csv('shein_urls.csv')
urls = pd.read_csv('shein_urls.csv', index_col=0)

import csv

csv_file=open('OpsCons_Shein_table.csv','w')
csv_writer = csv.writer(csv_file)
csv_writer.writerow(['id_style', 'display_name', 'materials', 'color', 'size', 'price', 'currency', 'product_url', 'image_links', 'brand_name', 'retailer', 'description', 'scrapped_date', 'high_level', 'low_level', 'gender', 'secondhand', 'shipping_from', 'style', 'availability'])
i=0
def extract_product_info(url):
        response = requests.get(url)

        results_page = Soup(response.content, 'html')
        scripts = results_page.find_all('script')
        for index, script in enumerate(scripts):
            text = script.text
            pattern = r'productIntroData: ([\s\S]*),\n[\s\S*]'
            product = re.findall(pattern, text)
            if product != []:
              test = product[0].replace('\n', '')
              d = demjson.decode(test)
              display_name = float('NaN')
              materials = float('NaN')
              color = float('NaN')
              size = float('NaN')
              availability = float('NaN')
              price = d['detail']['salePrice']['amount']
              currency = 'USD'
              product_url = url
              image_links = float('NaN')
              brand_name = 'Shein'
              retailer = 'Shein'
              description = float('NaN')
              scrapped_date = date.today()
              high_level = float('NaN')
              low_level = float('NaN')
              gender = float('NaN')
              secondhand = 'No'
              shipping_from = float('NaN')
              style = float('NaN')
              id_style=float('NaN')
              try:
                display_name = d['detail']['goods_name']
                materials = d['detail']['productDetails'][15]['attr_value']
                color = d['detail']['productDetails'][0]['attr_value']
                size = [d['sizeInfoDes']['sizeInfo'][i]['size'] for i in range(len(d['sizeInfoDes']['sizeInfo']))]
                image_links = [d['detail']['original_img'][2:]]
                description = d['metaInfo']['meta_description']
                high_level = d['currentCat']['cat_name']
                low_level = d['currentCat']['cat_name']
                gender = d['parentCats']['cat_name'].split(' ')[0]
                style = d['detail']['productDetails'][1]['attr_value']
              except Exception as e:
                pass

              csv_writer.writerow([id_style, display_name, materials, color, size, price, currency, product_url, image_links, brand_name, retailer, description, scrapped_date, high_level, low_level, gender, secondhand, shipping_from, style, availability])

url_list = urls['loc']
for url in url_list:
  extract_product_info(url)
csv_file.close()

"""## Data Cleaning

"""

import pandas as pd

df = pd.read_csv('OpsCons_Shein_table.csv')
df.drop_duplicates(keep = 'first', inplace=True)
df['color'] = df['color'].str.replace('[','{').str.replace(']','}')

GranCat = pd.read_csv("LowLevelCatagoryRegex.csv", header=None, skiprows=[0])

def process_cat(df):
    # get all the values in category regex
    subCats = GranCat.iloc[:, 1:]
    Cats = []
    for index, row in subCats.iterrows():
        for i in GranCat.columns[1:]:
            if pd.isnull(row[i]): continue
            else: Cats.append(row[i])
               
    # assign potential category for each clothing row based on title, else other
    clothesCat = []
    for index, row in df.iterrows():
        rowCat = float("NaN")
        for cat in Cats:
            if re.search(cat, row['display_name'].lower()):
                rowCat = cat
                print(rowCat)
                break
        if pd.isnull(rowCat):
            clothesCat.append('other')
        else:
            clothesCat.append(rowCat)

    return clothesCat

# set parent category to each product
def get_cat(x):
    if x == 'other': return x
    else: return [GranCat[0][i] for i in GranCat.index if x in GranCat.iloc[i].to_list()][0]

# get low_level column given a df
def get_lowlevel(df):
    df['low_level'] = process_cat(df)
    df['low_level'] = df['low_level'].apply(lambda x: get_cat(x))
    return df

df = get_lowlevel(df)

#High level

import re
from math import *
GranCat = pd.read_csv("HighLevelCatagory-LowLevelCatagory - HighLevelLowLevel.csv" , header=None, skiprows=[0])


def process_cat(df1,df2):
    subCats = GranCat.iloc[:, 0:2]
    for i in range(len(df2)):
      df1[i]=df2[i]
      for index2, row2 in subCats.iterrows():
        if df2[i]==row2[1]:
          df1[i]=row2[0]
    return df1
    
               

# get high_level column given a df
def get_highlevel(df):
    df['high_level']=process_cat(df['high_level'],df['low_level'])
    return df

df = get_highlevel(df)

##### get mapped_id
GranRegs = pd.read_csv('MaterialProxy.csv', header=None, skiprows=[0])
GranRegs = GranRegs.drop(columns=[1])
GranRegs.columns = list(range(len(GranRegs.columns)))
#extract materials

#extract materials

def extractMaterials(df):
    fabric_text = df['materials']
    fabric_text = str(fabric_text).replace('solvent', '').replace('Grown', '').replace('grown', '').replace('organically', 'organic').replace('Organically', 'organic')
    splitted_fabric = fabric_text.replace("'", '').replace("[", '').replace("]", '').replace(",", '').replace('’', '').replace('.', '').replace('–', '').replace('-', '').split()
    pattern = "\d{1,3}[$%]"
    
    description_text = str(df['description']) + str(df['display_name']) + str(df['materials'])
    description_text = str(description_text).replace('solvent', '').replace('Grown', '').replace('grown', '').replace('organically', 'organic').replace('Organically', 'organic')
    splitted_description = description_text.replace("'", '').replace("[", '').replace("]", '').replace(",", '').replace('’', '').replace('.', '').replace('–', '').replace('-', '')
    pattern = "\d{1,3}[$%]"
    
    mat_list = []
    extra_materials = []
    
    
    ## getting the materials regex matching
    subRegs = GranRegs.iloc[:, 1:]
    Regs = []
    materialsReg = []
    for index, row in subRegs.iterrows():
        for i in range(1,len(GranRegs.columns)):
            if pd.isnull(row[i]): continue
            else: Regs.append(row[i])
    total_percentage = [] 
    is_no_percentage = True 
    for i in range(len(splitted_fabric)):
        if re.fullmatch(pattern,splitted_fabric[i]):
            total_percentage.append(float(splitted_fabric[i].replace("%", '')))
            if sum(total_percentage) <= 100:
            
                if splitted_fabric[i+1].lower() == 'organic':
                    mat_list.append(splitted_fabric[i] + ' ' + splitted_fabric[i+1].lower() + ' ' + splitted_fabric[i+2].lower())
                    is_no_percentage = False
                    
                elif splitted_fabric[i+1].lower() == 'recycled':
                    mat_list.append(splitted_fabric[i] + ' ' + splitted_fabric[i+1].lower() + ' ' + splitted_fabric[i+2].lower())
                    is_no_percentage = False
                
                elif splitted_fabric[i+1].lower() == 'tencel™':
                    mat_list.append(splitted_fabric[i] + ' ' + splitted_fabric[i+1].lower() + ' ' + splitted_fabric[i+2].lower())
                    is_no_percentage = False
                    
                else:
                    if splitted_fabric[i] + ' ' + splitted_fabric[i+1].lower() not in mat_list:
                        mat_list.append(splitted_fabric[i] + ' ' + splitted_fabric[i+1].lower())
                        is_no_percentage = False
                        
    if sum(total_percentage) < 100 or is_no_percentage == True:
        for reg in Regs:
            #print(splitted_description.lower())
            if re.search(reg, splitted_description.lower()):
                match = re.search(reg, splitted_description.lower())
                matched_material = splitted_description[match.start():match.end()].lower()
                
                if matched_material not in extra_materials:
                    extra_materials.append(matched_material)
    
    #print(extra_materials)
    if len(extra_materials)>1:
        if "cotton" in extra_materials and "organic cotton" in extra_materials:
            mat_list.append("100% organic cotton")
        if "leather" in extra_materials:
            mat_list.append("100% leather")
    elif len(extra_materials)!=0:
        mat_list.append("100% " + extra_materials[0])

            
    
    return mat_list

df['materials'] = df.apply(lambda x: extractMaterials(x), axis=1)

df.to_csv('OpsCons_Shein_table.csv', encoding='utf-8-sig')

df