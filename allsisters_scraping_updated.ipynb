{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllSisters Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\linds\\anaconda3\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "import time, urllib, json, unicodedata\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Headers to imitate browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Mobile Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'DNT': '1',\n",
    "    'Referer': 'https://www.google.co.uk',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_URL = 'https://allsisters.com'\n",
    "URL_extension_for_products = '/products.json?limit=250&page='\n",
    "current_page_number = 1\n",
    "\n",
    "all_URLs_and_JSON = {}\n",
    "current_product_list = None\n",
    "\n",
    "while current_product_list != []:\n",
    "    # make the full json url\n",
    "    url = main_URL + URL_extension_for_products + str(current_page_number)\n",
    "    #print(url)\n",
    "    \n",
    "    # Get the page\n",
    "    req = urllib.request.Request(url, headers=headers)\n",
    "    with urllib.request.urlopen(req) as request:\n",
    "        s = request.read()\n",
    "        response = json.loads(s)\n",
    "        current_product_list = response['products']\n",
    "    \n",
    "    time.sleep(randint(4,6))\n",
    "    \n",
    "    # construct the URL\n",
    "    for product in current_product_list:\n",
    "        product_url = main_URL + '/products/' + product['handle']\n",
    "        all_URLs_and_JSON[product_url] = product\n",
    "        \n",
    "    current_page_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "print(len(all_URLs_and_JSON))\n",
    "#all_URLs_and_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_product_name(soup, product_JSON):\n",
    "    return product_JSON['title'].strip()\n",
    "\n",
    "def find_material(soup, product_JSON):\n",
    "    materials = {}\n",
    "    materials_container = soup.find('div', id = 'desc').find_all('p')\n",
    "    materials_container = [item.text.strip() for item in materials_container if item.text.strip() != '']\n",
    "    materials_list = []\n",
    "    \n",
    "    for item in materials_container:\n",
    "        print(item)\n",
    "        if item[0].isdigit():\n",
    "            materials_list = item.split('-')\n",
    "\n",
    "    materials_list = [item.strip().replace('%', '% &ND') for item in materials_list]\n",
    "    materials_split = [item.split('&ND') for item in materials_list]\n",
    "\n",
    "    for item in materials_split:\n",
    "        materials[item[1].strip().lower()] = item[0].strip()\n",
    "            \n",
    "    return materials\n",
    "\n",
    "def find_color(soup, product_JSON, the_description):\n",
    "    \n",
    "    product_title = product_JSON['title'].strip().lower()\n",
    "    print(product_title)\n",
    "    \n",
    "    if 'black' in product_title:\n",
    "        color = ['black']\n",
    "    elif 'white' in product_title:\n",
    "        color = ['white']\n",
    "    elif 'colors' in product_title:\n",
    "        color = ['multi-colored']\n",
    "    elif len(product_JSON['options']) == 2:\n",
    "        if product_JSON['options'][1]['name'] == 'Color':   \n",
    "            color_container = product_JSON['options'][1]\n",
    "            if color_container['name'] == 'Color':\n",
    "                color = color_container['values']\n",
    "                color = [item.lower() for item in color]\n",
    "    elif 'black' and 'white' in the_description[0].lower():\n",
    "        color = ['black and white']\n",
    "    elif 'black' in the_description[0].lower():\n",
    "        color = ['black']\n",
    "    elif 'white' in the_description[0].lower():\n",
    "        color = ['white']\n",
    "    else: \n",
    "        color = None\n",
    "        \n",
    "    return color\n",
    "\n",
    "def find_price(soup, product_JSON):\n",
    "    price_text = soup.find('h2', id = 'price-preview').text.strip().split(' ')\n",
    "    price = (price_text[0], price_text[1])\n",
    "    return price\n",
    "\n",
    "def find_size(soup, product_JSON):\n",
    "    return [i.text.strip() for i in soup.find('div', {'class': 'swatch-variant product-type-items swatch clearfix'}).find_all('div')][1:]\n",
    "\n",
    "def find_image(soup, product_JSON):\n",
    "    image_urls_container = product_JSON['images']\n",
    "    image_urls = [item['src'] for item in image_urls_container]\n",
    "    return image_urls\n",
    "\n",
    "def find_brand(soup, product_JSON):\n",
    "    return product_JSON['vendor']\n",
    "\n",
    "def find_description(soup, product_JSON):\n",
    "    description_container = soup.find('div', id = 'desc').find_all('p')\n",
    "    description = []\n",
    "    for item in description_container:\n",
    "        if item.text[0].isdigit():\n",
    "            break\n",
    "        else:\n",
    "            description_section = item.text.strip()\n",
    "            if description_section != '':\n",
    "                description.append(unicodedata.normalize(\"NFKD\", description_section))\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_product(URL, html_soup, product_JSON):\n",
    "    product = {}\n",
    "    \n",
    "    the_description = find_description(html_soup, product_JSON)\n",
    "\n",
    "    product['Name'] = find_product_name(html_soup, product_JSON)\n",
    "    product['Material'] = find_material(html_soup, product_JSON)\n",
    "    product['Color'] = find_color(html_soup, product_JSON, the_description)\n",
    "    product['Price'] = find_price(html_soup, product_JSON)\n",
    "    product['Size'] = find_size(html_soup, product_JSON)\n",
    "    product['URL'] = URL\n",
    "    product['Image'] = find_image(html_soup, product_JSON)\n",
    "    product['Brand_name'] = find_brand(html_soup, product_JSON)\n",
    "    product['Description'] = the_description\n",
    "    \n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_available(product_JSON):\n",
    "    variants = product_JSON['variants']\n",
    "    \n",
    "    is_available = False\n",
    "    \n",
    "    for variant in variants:\n",
    "        availability = variant['available']\n",
    "        if availability == True:\n",
    "            is_available = True\n",
    "    \n",
    "    return is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 3 cells scrape a single product\n",
    "# URL_to_scrape = 'https://allsisters.com/products/serena-leggins'\n",
    "# page = requests.get(URL_to_scrape, headers = headers)\n",
    "# single_soup = bs(page.content, 'html.parser')\n",
    "\n",
    "# product_as_json = None\n",
    "# URL_to_scrape_JSON = URL_to_scrape + \".json\"\n",
    "\n",
    "# single_req = urllib.request.Request(URL_to_scrape_JSON, headers=headers)\n",
    "# with urllib.request.urlopen(single_req) as request:\n",
    "#     s = request.read()\n",
    "#     response = json.loads(s)\n",
    "#     product_as_json = response['product']\n",
    "#     #print(product_as_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# singleProductResult  = [scrape_single_product(URL_to_scrape, single_soup, product_as_json)]\n",
    "\n",
    "# #print(singleProductResult)\n",
    "\n",
    "# product_data_frame = pd.DataFrame(data = singleProductResult,)\n",
    "\n",
    "# product_data_frame.to_csv('single_item.csv', encoding='utf-8-sig')\n",
    "\n",
    "# pd.DataFrame(data = singleProductResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [01:30<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "all_products = []\n",
    "failed_urls = []\n",
    "unavailable_products = []\n",
    "\n",
    "# counter = 0\n",
    "\n",
    "for url, the_JSON in tqdm(all_URLs_and_JSON.items()):\n",
    "    try:\n",
    "       # print(url)\n",
    "        if is_available(the_JSON):\n",
    "            #print(is_available(the_JSON))\n",
    "            page = requests.get(url, headers = headers)\n",
    "            soup = bs(page.content, 'html.parser')\n",
    "\n",
    "            product = scrape_single_product(url, soup, the_JSON)\n",
    "            #print(product)\n",
    "            if product['Material']:\n",
    "                all_products.append(product)\n",
    "        else:\n",
    "            unavailable_products.append(url)\n",
    "        time.sleep(randint(2,5))\n",
    "    except:\n",
    "        failed_urls.append(url)\n",
    "    \n",
    "#     if counter == 20:\n",
    "#         break\n",
    "#     counter += 1\n",
    "\n",
    "# print('\\nSummary\\n')\n",
    "# print(len(failed_urls), 'Failed URLs: ', failed_urls)\n",
    "# print(len(unavailable_products), 'Unavailable Products: ', unavailable_products)\n",
    "# print('Total products collected: ', len(all_products))\n",
    "# print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_again_URLs = []\n",
    "for url in failed_urls:\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        \n",
    "        with urllib.request.urlopen(req) as request:\n",
    "            page = request.read()\n",
    "            soup = bs(page, 'html.parser')\n",
    "            all_products.append(scrape_single_product(url, soup))\n",
    "            \n",
    "            time.sleep(randint(5,15))\n",
    "            \n",
    "    except:\n",
    "        fail_again_URLs.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data_frame = pd.DataFrame(data = all_products)\n",
    "product_data_frame.to_csv('allsisters_table.csv', encoding='utf-8-sig')\n",
    "pd.DataFrame(data = all_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b6edf1972e686903e8d188d8727c93eb73c598b12c20b8d76c153accc2b22b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
